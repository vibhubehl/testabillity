{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.preprocessing import QuantileTransformer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, median_absolute_error, r2_score\n",
    "from sklearn.model_selection import ShuffleSplit, GridSearchCV\n",
    "\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn import linear_model\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "from sklearn.ensemble import GradientBoostingRegressor, RandomForestRegressor, HistGradientBoostingRegressor\n",
    "from sklearn.ensemble import VotingRegressor\n",
    "from sklearn.preprocessing import MinMaxScaler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Decription"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook implements the models used in the testability research paper. It also uses the best hyperparameters which are highlighted in the paper and then makes the voting regressor and calculates accuracy on the our dataset. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset and Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"./dataset/researchDataset/DS07012.csv\")\n",
    "\n",
    "data.drop('Class', axis=1, inplace=True)\n",
    "corr_matrix = data.corr().abs()\n",
    "upper = corr_matrix.where(np.triu(np.ones(corr_matrix.shape), k=1).astype(bool))\n",
    "to_drop = [column for column in upper.columns if any(upper[column] > 0.95)]\n",
    "data.drop(to_drop, axis=1, inplace=True)\n",
    "data.columns\n",
    "\n",
    "df = data.copy()\n",
    "label = df[\"Testability\"]\n",
    "df.drop('Testability', axis=1, inplace=True)\n",
    "scaler = MinMaxScaler()\n",
    "scaler.fit(df)\n",
    "df = scaler.transform(df)\n",
    "df = pd.DataFrame(df)\n",
    "\n",
    "(X_train, X_test, y_train, y_test) = train_test_split(df, label, random_state=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Histogram based Gradient Boost Regressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HGBR MAE: 0.11803233456348623\n",
      "HGBR RMSE: 0.16526973171232745\n",
      "HGBR MedAE: 0.08379819776077951\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import HistGradientBoostingRegressor\n",
    "\n",
    "clf = HistGradientBoostingRegressor(loss='squared_error', max_depth=18, min_samples_leaf=15, max_iter=500).fit(X_train, y_train)\n",
    "y_pred = clf.predict(X_test)\n",
    "print('HGBR MAE:',mean_absolute_error(y_test, y_pred))\n",
    "print('HGBR RMSE:',mean_squared_error(y_test, y_pred, squared = False))\n",
    "print('HGBR MedAE:',median_absolute_error(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear Regressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Linear MAE: 0.18599290180846545\n",
      "Linear RMSE: 0.22963908053226662\n",
      "Linear MedAE: 0.15822532071600456\n"
     ]
    }
   ],
   "source": [
    "from sklearn import linear_model\n",
    "\n",
    "clf = linear_model.SGDRegressor(loss='huber', penalty='l2', learning_rate='invscaling', max_iter=50).fit(X_train, y_train) \n",
    "y_pred = clf.predict(X_test)\n",
    "print('Linear MAE:',mean_absolute_error(y_test, y_pred))\n",
    "print('Linear RMSE:',mean_squared_error(y_test, y_pred, squared = False))\n",
    "print('Linear MedAE:',median_absolute_error(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SVMR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SVMR MAE: 0.13224962304813254\n",
      "SVMR RMSE: 0.18007823900341396\n",
      "SVMR MedAE: 0.09089355855852835\n"
     ]
    }
   ],
   "source": [
    "from sklearn.svm import NuSVR\n",
    "\n",
    "clf = NuSVR(kernel='rbf', nu=0.5).fit(X_train, y_train)\n",
    "y_pred = clf.predict(X_test)\n",
    "print('SVMR MAE:',mean_absolute_error(y_test, y_pred))\n",
    "print('SVMR RMSE:',mean_squared_error(y_test, y_pred, squared = False))\n",
    "print('SVMR MedAE:',median_absolute_error(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DTR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DTR MAE: 0.14428506530469518\n",
      "DTR RMSE: 0.1961454402014521\n",
      "DTR MedAE: 0.10713561013983577\n"
     ]
    }
   ],
   "source": [
    "regressor = DecisionTreeRegressor(criterion='squared_error', max_depth=8, min_samples_leaf=28).fit(X_train, y_train)\n",
    "y_pred = regressor.predict(X_test)\n",
    "print('DTR MAE:',mean_absolute_error(y_test, y_pred))\n",
    "print('DTR RMSE:',mean_squared_error(y_test, y_pred, squared = False))\n",
    "print('DTR MedAE:',median_absolute_error(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RFR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RFR MAE: 0.11975334299350378\n",
      "RFR RMSE: 0.16878253254772543\n",
      "RFR MedAE: 0.08354096548803652\n"
     ]
    }
   ],
   "source": [
    "regressor = RandomForestRegressor(n_estimators=150, max_depth=28, min_samples_leaf=2, criterion='squared_error').fit(X_train, y_train)\n",
    "y_pred = regressor.predict(X_test)\n",
    "print('RFR MAE:',mean_absolute_error(y_test, y_pred))\n",
    "print('RFR RMSE:',mean_squared_error(y_test, y_pred, squared = False))\n",
    "print('RFR MedAE:',median_absolute_error(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MLPR MAE: 0.14337790864984926\n",
      "MLPR RMSE: 0.19807072853096983\n",
      "MLPR MedAE: 0.1024361991005591\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Python311\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:686: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "regressor = MLPRegressor(random_state=7, hidden_layer_sizes=(512, 256, 100), activation='tanh', learning_rate='constant').fit(X_train, y_train)\n",
    "y_pred = regressor.predict(X_test)\n",
    "print('MLPR MAE:',mean_absolute_error(y_test, y_pred))\n",
    "print('MLPR RMSE:',mean_squared_error(y_test, y_pred, squared = False))\n",
    "print('MLPR MedAE:',median_absolute_error(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Votting Regression of RFR, HGBR and MLPR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Python311\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:686: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Voting MAE: 0.11785408369593284\n",
      "Voting RMSE: 0.16483762518122372\n",
      "Voting MedAE: 0.08277470633180838\n"
     ]
    }
   ],
   "source": [
    "regressor1 = MLPRegressor(random_state=7, hidden_layer_sizes=(512, 256, 100), activation='tanh', learning_rate='constant')\n",
    "regressor2 = RandomForestRegressor(n_estimators=150, max_depth=28, min_samples_leaf=2, criterion='squared_error')\n",
    "regressor3 = HistGradientBoostingRegressor(loss='squared_error', max_depth=18, min_samples_leaf=15, max_iter=500)\n",
    "er = VotingRegressor([('MLP', regressor1), ('RFR', regressor2), ('HGBR', regressor3)])\n",
    "\n",
    "er.fit(X_train, y_train)\n",
    "y_pred = er.predict(X_test)\n",
    "print('Voting MAE:',mean_absolute_error(y_test, y_pred))\n",
    "print('Voting RMSE:',mean_squared_error(y_test, y_pred, squared = False))\n",
    "print('Voting MedAE:',median_absolute_error(y_test, y_pred))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
